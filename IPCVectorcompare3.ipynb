{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrkf3a0ejbQ59K97VH7mUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharathc21/ML/blob/main/IPCVectorcompare3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfIfbsifwDVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8axGX0lIoOES",
        "outputId": "ce9ef090-32d8-4ab6-a5f2-c72e5d978267"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0)\n",
            "Requirement already satisfied: loguru>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.7.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.26.16)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7lSGaK8olQJ",
        "outputId": "0fa08326-cf53-4bc9-f6fa-bff1d214073c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIRZLvjDoriU",
        "outputId": "4d8336db-efed-4054-853d-274edc8909ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vKruG4-nw0B",
        "outputId": "c432762b-fdca-48b8-fb07-608bd655cac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pinecone\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, model_type=\"huggingface\"):\n",
        "    if model_type == \"huggingface\":\n",
        "        # Hugging Face Transformers for summarization\n",
        "        tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "        inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        outputs = model.generate(inputs, max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    elif model_type == \"openai\":\n",
        "        # OpenAI GPT-3 for summarization\n",
        "        openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "        response = openai.Completion.create(\n",
        "            engine=\"text-davinci-003\",\n",
        "            prompt=\"Summarize the following text: \\n\" + text,\n",
        "            max_tokens=100,\n",
        "            temperature=0.5,\n",
        "            top_p=1.0,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "        )\n",
        "        summary = response.choices[0].text.strip()\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "NGk98sPxoFEg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY', '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')\n",
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '2fcfada8-7b03-4495-9f74-aeafbd06fd2f')\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'us-west1-gcp-free')"
      ],
      "metadata": {
        "id": "TpJlaDYYoyun"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_API_ENV  # next to api key in console\n",
        ")\n",
        "index_name = \"ipclangchain\" # put-in the name of pinecone index here\n",
        "# Connect to Pinecone index\n",
        "pinecone_index = pinecone.Index(index_name)"
      ],
      "metadata": {
        "id": "Rh-C-O2To00N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder path containing the text files\n",
        "# folder_path = \"C:\\\\Users\\\\shara\\\\OneDrive\\\\Desktop\\\\Projects\\\\classification_service\\\\Thesis\\\\IK_Sharath\\\\data\\\\temp\\\\\"\n"
      ],
      "metadata": {
        "id": "W6xWYRbYo2h3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# def getLocalFiles():\n",
        "#     _files = files.upload()\n",
        "#     if len(_files) >0:\n",
        "#        for k,v in _files.items():\n",
        "#          open(k,'wb').write(v)\n",
        "# getLocalFiles()"
      ],
      "metadata": {
        "id": "_xJVmm9rp7Hb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/'\n",
        "os.listdir(folder_path)\n",
        "len(os.listdir(folder_path))\n",
        "# /content/2425205.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-bbtY9mo4pk",
        "outputId": "a61b2d6d-a47d-4030-8fc4-d4bb67d977fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRB9eZIdtgSf",
        "outputId": "3be4c333-ba34-4fcd-8a96-52332ed1d44f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " '4199416 (1).txt',\n",
              " '4990844.txt',\n",
              " '2425205 (1).txt',\n",
              " '2958232.txt',\n",
              " '4199416.txt',\n",
              " '4990844 (1).txt',\n",
              " '2958232 (1).txt',\n",
              " '3547017 (1).txt',\n",
              " '2425205.txt',\n",
              " '3547017.txt',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duGSyEUxs4P6",
        "outputId": "7ea46762-07cd-46fb-8a2c-febd9d69d594"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# ! pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "VNcGK0zquj1z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dJS96Yb1o7Sx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datsets transformers[sentencepiece]\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjXSkxDKvQh3",
        "outputId": "9d034312-b61e-4f7f-e00d-bd51ca9ab321"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datsets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for datsets\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in os.listdir():\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(\"/content/\", filename)\n",
        "\n",
        "        # Read the text file\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Summarize the text using Hugging Face Transformers\n",
        "        huggingface_summary = summarize_text(text, model_type=\"huggingface\")\n",
        "\n",
        "        # Summarize the text using OpenAI GPT-3\n",
        "        # openai_summary = summarize_text(text, model_type=\"openai\")\n",
        "\n",
        "\n",
        "        # Load a pre-trained Sentence Transformer model\n",
        "        model2 = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "\n",
        "        # Generate the embedding for the Hugging Face summary\n",
        "        # summary_embedding = model2.encode([huggingface_summary])[0]\n",
        "        # # Convert the embedding to a list\n",
        "        # summary_values = summary_embedding.tolist()\n",
        "\n",
        "\n",
        "\n",
        "        summary_embedding = model2.encode([huggingface_summary], convert_to_tensor=True, device='cuda',  normalize_embeddings=True)[0]\n",
        "\n",
        "        # Convert the embedding to a numpy array\n",
        "        # summary_values = summary_embedding.tolist()\n",
        "        # Pad the embedding with zeros to match the dimensionality of the index vectors\n",
        "        # summary_values = summary_embedding.cpu().numpy().tolist()\n",
        "        padding = np.zeros(1536 - summary_embedding.size()[0])\n",
        "        summary_values = np.concatenate((summary_embedding.cpu().numpy(), padding)).tolist()\n",
        "\n",
        "\n",
        "        # Perform similarity search with Pinecone\n",
        "        query_vector_hf = pinecone.Vector(id='query', values= summary_values,  dimension=1536)\n",
        "        # query_vector_openai = pinecone.Vector(openai_summary)\n",
        "\n",
        "        results_hf = pinecone_index.query(queries=[summary_values], top_k=3)\n",
        "        # results_openai = pinecone_index.query(queries=[query_vector_openai])\n",
        "\n",
        "        # Process the search results for Hugging Face summary\n",
        "        for result in results_hf.results:\n",
        "            document_id = result.id\n",
        "            similarity_score = result.score\n",
        "            # Do something with the document ID and similarity score\n",
        "\n",
        "        # Process the search results for OpenAI summary\n",
        "        # for result in results_openai[0].results:\n",
        "        #     document_id = result.id\n",
        "        #     similarity_score = result.score\n",
        "            # Do something with the document ID and similarity score\n",
        "\n",
        "\n",
        "\n",
        "# # Disconnect from Pinecone\n",
        "# pinecone.uninit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QBx8cQ5s2Yg",
        "outputId": "6dca7c6b-4c3a-41e4-b486-987728bc615c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ot7hMePzt-S3"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}